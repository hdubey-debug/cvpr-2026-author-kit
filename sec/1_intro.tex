\section{Introduction}
\label{sec:intro}

The rapid advancement of Video Language Models (VLMs) has demonstrated remarkable capabilities in understanding and describing visual content, particularly in video analysis tasks. Current state-of-the-art models such as VideoLLaMA3~\cite{cheng2024videollama2}, InternVL~\cite{chen2024internvl}, and mPLUG-Owl3~\cite{ye2024mplugowl3} have achieved impressive performance across various video understanding benchmarks. However, evaluating the quality of long-form, paragraph-level video descriptions generated by these models remains a significant challenge, particularly when assessing their ability to capture the overarching story and fine-grained details.

Traditional evaluation methods for video description tasks have predominantly relied on surface-level textual similarity metrics such as BLEU~\cite{papineni2002bleu}, ROUGE~\cite{lin2004rouge}, and METEOR~\cite{banerjee2005meteor}. While these metrics provide useful insights into lexical overlap between generated and reference descriptions, they often fail to capture the semantic understanding and discourse structure that are crucial for comprehensive video understanding. To move beyond purely local n-gram matching, CIDEr~\cite{vedantam2015cider} and SODA~\cite{soda} have been proposed to place greater emphasis on informative content and event structure rather than raw lexical overlap. However, CIDEr remains fundamentally anchored to n-gram statistics and may still underrepresent deeper semantic and temporal coherence, while SODA depends on intermediate event representations whose quality can be brittle in realistic video settings. Recent advances in evaluation methodologies have introduced embedding-based similarity measures~\cite{zhang2019bertscore,lee2024nv} and LLM-as-judge frameworks~\cite{liu2023g}; yet LLM-as-judge approaches are typically black-box and their scores can exhibit limited or unstable correlation with human judgments, particularly for long-form video descriptions, and comprehensive benchmarks that systematically compare these diverse evaluation approaches for long-form video description remain limited.


Current video-language benchmarks present several fundamental limitations for evaluating long-form, paragraph-level video descriptions. Single-sentence captioning datasets like MSVD~\cite{chen2011msvd}, MSR-VTT~\cite{xu2016msr}, and VATEX~\cite{wang2019vatex} focus on describing isolated short clips without requiring comprehensive multi-event understanding. Dense captioning benchmarks such as ActivityNet Captions~\cite{krishna2017dense} and YouCook2~\cite{zhou2018youcook2} provide multi-sentence descriptions but evaluate segments independently, failing to assess holistic video understanding. Video QA benchmarks like TVQA~\cite{lei2018tvqa}, NExT-QA~\cite{xiao2021next}, and Video-MME~\cite{fu2024videomme} test discrete reasoning questions rather than the ability to generate comprehensive descriptions. Furthermore, many datasets include proper nouns and specific cultural references that can lead to spurious correlations and may not accurately reflect a model's fundamental video understanding capabilities.

To address these limitations, we introduce CLIP-CC-Bench, a comprehensive evaluation suite for long-form, paragraph-level video description. Our benchmark comprises 200 carefully selected 90-second movie clips with expert-written paragraph descriptions that systematically exclude proper nouns and cultural references, ensuring evaluation focuses on fundamental visual understanding rather than memorized associations. We make four key contributions:

\textbf{(1) Curated bias-minimizing dataset:} Our clips span diverse cinematographic styles and temporal dynamics, providing challenging evaluation scenarios while eliminating spurious correlations from proper nouns and specific cultural references.

\textbf{(2) Systematic evaluation framework:} We conduct the first comprehensive comparison of three evaluation families (N-gram metrics, embedding-based similarity, and LLM-as-judge) for paragraph-level video description, evaluating 14 state-of-the-art VLMs across multiple assessment dimensions.

\textbf{(3) Empirical insights:} Our analysis reveals that embedding-based and LLM-as-judge approaches provide significantly more nuanced assessment than traditional N-gram metrics, with important implications for future model development and evaluation methodologies.

\textbf{(4) Public benchmark release:} We provide all data, evaluation code, and baseline results to facilitate reproducible research and continued advancement in video understanding evaluation.
