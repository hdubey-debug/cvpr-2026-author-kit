\section{Introduction}
\label{sec:intro}

The rapid advancement of Vision Language Models (VLMs) has demonstrated remarkable capabilities in understanding and describing visual content, particularly in video analysis tasks. Current state-of-the-art models such as VideoLLaMA3~\cite{cheng2024videollama2}, InternVL~\cite{chen2024internvl}, and mPLUG-Owl3~\cite{ye2024mplugowl3} have achieved impressive performance across various video understanding benchmarks. However, evaluating the quality of long-form, paragraph-level video descriptions generated by these models remains a significant challenge, particularly when assessing their ability to capture fine-grained temporal dynamics and maintain comprehensive visual understanding.

Traditional evaluation methods for video description tasks have predominantly relied on surface-level textual similarity metrics such as BLEU~\cite{papineni2002bleu}, ROUGE~\cite{lin2004rouge}, and METEOR~\cite{banerjee2005meteor}. While these metrics provide useful insights into lexical overlap between generated and reference descriptions, they often fail to capture semantic understanding that is crucial for comprehensive video understanding. Recent advances in evaluation methodologies have introduced embedding-based similarity measures~\cite{zhang2019bertscore,lee2024nv} and LLM-as-judge frameworks~\cite{liu2023g}, yet comprehensive benchmarks that systematically compare these diverse evaluation approaches for long-form video description remain limited.

Current video-language benchmarks present several fundamental limitations for evaluating long-form, paragraph-level video descriptions. Single-sentence captioning datasets like MSVD~\cite{chen2011msvd}, MSR-VTT~\cite{xu2016msr}, and VATEX~\cite{wang2019vatex} focus on describing isolated short clips without requiring comprehensive multi-event understanding. Dense captioning benchmarks such as ActivityNet Captions~\cite{krishna2017dense} and YouCook2~\cite{zhou2018youcook2} provide multi-sentence descriptions but evaluate segments independently, failing to assess holistic video understanding. Video QA benchmarks like TVQA~\cite{lei2018tvqa}, NExT-QA~\cite{xiao2021next}, and Video-MME~\cite{fu2024videomme} test discrete reasoning questions rather than the ability to generate comprehensive descriptions. Furthermore, many datasets include proper nouns and specific cultural references that can lead to spurious correlations and may not accurately reflect a model's fundamental video understanding capabilities.

To address these limitations, we introduce CLIP-CC-Bench, a comprehensive evaluation suite for long-form, paragraph-level video description. Our benchmark comprises 200 carefully selected 90-second movie clips with expert-written paragraph descriptions that systematically exclude proper nouns and cultural references, ensuring evaluation focuses on fundamental visual understanding rather than memorized associations. We make four key contributions:

\textbf{(1) Curated bias-minimizing dataset:} Our clips span diverse cinematographic styles and temporal dynamics, providing challenging evaluation scenarios while eliminating spurious correlations from proper nouns and specific cultural references.

\textbf{(2) Systematic evaluation framework:} We conduct the first comprehensive comparison of three evaluation families (N-gram metrics, embedding-based similarity, and LLM-as-judge) for paragraph-level video description, evaluating 14 state-of-the-art VLMs across multiple assessment dimensions.

\textbf{(3) Empirical insights:} Our analysis reveals that embedding-based and LLM-as-judge approaches provide significantly more nuanced assessment than traditional N-gram metrics, with important implications for future model development and evaluation methodologies.

\textbf{(4) Public benchmark release:} We provide all data, evaluation code, and baseline results to facilitate reproducible research and continued advancement in video understanding evaluation.
