\section{Introduction}
\label{sec:intro}

The rapid advancement of Video Language Models (VLMs) has demonstrated remarkable capabilities in understanding and describing visual content, particularly in video analysis tasks. Current state-of-the-art models such as VideoLLaMA3~\cite{cheng2024videollama2}, InternVL~\cite{chen2024internvl}, and mPLUG-Owl3~\cite{ye2024mplugowl3} have achieved impressive performance across various video understanding benchmarks. However, evaluating the quality of long-form, paragraph-level video descriptions generated by these models remains a significant challenge, particularly when assessing their ability to capture the overarching story and fine-grained details.

Traditional evaluation methods for video description tasks have predominantly relied on surface-level textual similarity metrics such as BLEU~\cite{papineni2002bleu}, ROUGE~\cite{lin2004rouge}, and METEOR~\cite{banerjee2005meteor}. While these metrics provide useful insights into lexical overlap between generated and reference descriptions, they often fail to capture the semantic understanding and discourse structure that are crucial for comprehensive video understanding. To move beyond purely local n-gram matching, CIDEr~\cite{vedantam2015cider} and SODA~\cite{fujita2020soda} have been proposed to place greater emphasis on informative content and event structure rather than raw lexical overlap. However, CIDEr remains fundamentally anchored to n-gram statistics and may still underrepresent deeper semantic and temporal coherence, while SODA relies on intermediate event representations with temporal boundaries and employs intersection-over-union calculations between predicted and reference timestamps. This dependency limits SODA's applicability to realistic long-form videos where multiple actions occur within brief temporal windows. Furthermore, SODA uses METEOR for event description similarity, which remains constrained to n-gram matching, and does not assess story-level comprehension across the entire video narrative. Recent advances in evaluation methodologies have introduced embedding-based similarity measures~\cite{zhang2019bertscore,lee2024nv} and LLM-as-judge frameworks~\cite{liu2023g}; yet LLM-as-judge approaches are typically black-box and exhibit significant consistency issues, with their scores showing limited or unstable correlation with human judgments, particularly for long-form video descriptions. Comprehensive benchmarks that systematically address these evaluation challenges for long-form video description remain limited.


Current video-language benchmarks present several fundamental limitations for evaluating long-form, paragraph-level video descriptions. Single-sentence captioning datasets like MSVD~\cite{chen2011msvd}, MSR-VTT~\cite{xu2016msr}, and VATEX~\cite{wang2019vatex} focus on describing isolated short clips without requiring comprehensive multi-event understanding. Dense captioning benchmarks such as ActivityNet Captions~\cite{krishna2017dense} and YouCook2~\cite{zhou2018youcook2} provide multi-sentence descriptions but evaluate segments independently, failing to assess holistic video understanding. Video QA benchmarks like TVQA~\cite{lei2018tvqa}, NExT-QA~\cite{xiao2021next}, and Video-MME~\cite{fu2024videomme} test discrete reasoning questions rather than the ability to generate comprehensive descriptions. Furthermore, many datasets include proper nouns and specific cultural references that can lead to spurious correlations and may not accurately reflect a model's fundamental video understanding capabilities.

To address these limitations, we introduce CLIP-CC-Bench, a comprehensive evaluation suite for long-form, paragraph-level video description. Our benchmark comprises of 5 hours of movie content segmented into 200 carefully selected 90-second movie clips with expert-written paragraph descriptions that systematically exclude proper nouns and cultural references, ensuring evaluation focuses on fundamental visual understanding rather than memorized associations. We make three key contributions:

\textbf{(1) Curated bias-minimizing dataset:} Our clips span diverse cinematographic styles and temporal dynamics, providing challenging evaluation scenarios while eliminating spurious correlations from proper nouns and specific cultural references.

\textbf{(2) Ensemble-based evaluation framework:} We employ an ensemble of five state-of-the-art LLM-based embedding models from the MTEB~\cite{muennighoff2023mteb} leaderboard to mitigate single-model bias and enhance evaluation reliability. Our framework applies two complementary methodologies: coarse-grained semantic matching for holistic paragraph-level alignment and fine-grained semantic matching for detailed content verification. We evaluate 17 state-of-the-art VLMs and report both Borda-aggregated rankings and average scores across evaluation dimensions.

\textbf{(3) Public benchmark release:} We provide all data, evaluation code, and baseline results to facilitate reproducible research and continued advancement in video understanding evaluation.
