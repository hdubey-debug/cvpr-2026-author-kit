\section{Results}
\label{sec:results}

We present comprehensive evaluation results for 17 state-of-the-art Vision Language Models on CLIP-CC-Bench using our ensemble-based evaluation framework. Our methodology employs five leading LLM-based embedding models from the MTEB~\cite{muennighoff2023mteb} leaderboard: GTE-Qwen2-7B~\cite{li2024gte}, KaLM-Embedding-Gemma3-12B~\cite{kalm2024}, Llama-Embed-Nemotron-8B~\cite{nvidia2024nemotron}, nv-embed-v2~\cite{lee2024nv}, and Qwen3-Embedding-8B~\cite{qwen2024embedding}. For each VLM, we compute coarse-grained semantic similarity (paragraph-level alignment) and fine-grained semantic similarity (sentence-level F1 matching) across all five embedding models. We then rank VLMs using Borda count based on their harmonic mean (HM) of coarse and fine scores for each embedding model, and report mean HM scores averaged across all five judges.

\subsection{Per-Judge Evaluation Statistics}

\Cref{tab:detailed_stats} presents detailed performance statistics for all 17 VLMs across the five embedding models. For each embedding model, we report coarse-grained similarity scores, fine-grained F1 scores, and their harmonic mean, along with standard deviations. The hierarchical structure reveals how different embedding models assess video description quality with varying sensitivities.

\begin{table*}[t]
\centering
\caption{Detailed evaluation statistics across five MTEB embedding models. For each model, we report Coarse (coarse-grained similarity), Fine (fine-grained F1), and HM (harmonic mean) with their standard deviations.}
\label{tab:detailed_stats}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|rrrrrr|rrrrrr|rrrrrr|rrrrrr|rrrrrr}
\toprule
& \multicolumn{6}{c|}{\textbf{GTE-Qwen2-7B}} & \multicolumn{6}{c|}{\textbf{KaLM-Gemma3-12B}} & \multicolumn{6}{c|}{\textbf{Nemo-8B}} & \multicolumn{6}{c|}{\textbf{NV-Embed-v2}} & \multicolumn{6}{c}{\textbf{Qwen3-8B}} \\
\textbf{VLM} & \rotatebox{90}{\textbf{Coarse}} & \rotatebox{90}{\textbf{C-Std}} & \rotatebox{90}{\textbf{Fine}} & \rotatebox{90}{\textbf{F-Std}} & \rotatebox{90}{\textbf{HM}} & \rotatebox{90}{\textbf{HM-Std}} & \rotatebox{90}{\textbf{Coarse}} & \rotatebox{90}{\textbf{C-Std}} & \rotatebox{90}{\textbf{Fine}} & \rotatebox{90}{\textbf{F-Std}} & \rotatebox{90}{\textbf{HM}} & \rotatebox{90}{\textbf{HM-Std}} & \rotatebox{90}{\textbf{Coarse}} & \rotatebox{90}{\textbf{C-Std}} & \rotatebox{90}{\textbf{Fine}} & \rotatebox{90}{\textbf{F-Std}} & \rotatebox{90}{\textbf{HM}} & \rotatebox{90}{\textbf{HM-Std}} & \rotatebox{90}{\textbf{Coarse}} & \rotatebox{90}{\textbf{C-Std}} & \rotatebox{90}{\textbf{Fine}} & \rotatebox{90}{\textbf{F-Std}} & \rotatebox{90}{\textbf{HM}} & \rotatebox{90}{\textbf{HM-Std}} & \rotatebox{90}{\textbf{Coarse}} & \rotatebox{90}{\textbf{C-Std}} & \rotatebox{90}{\textbf{Fine}} & \rotatebox{90}{\textbf{F-Std}} & \rotatebox{90}{\textbf{HM}} & \rotatebox{90}{\textbf{HM-Std}} \\
\midrule
VideoLLaMA3 & 0.75 & 0.05 & 0.64 & 0.04 & 0.69 & 0.04 & 0.82 & 0.05 & 0.76 & 0.03 & 0.79 & 0.03 & 0.68 & 0.07 & 0.57 & 0.04 & 0.62 & 0.05 & 0.68 & 0.10 & 0.47 & 0.07 & 0.55 & 0.08 & 0.72 & 0.06 & 0.69 & 0.03 & 0.70 & 0.04 \\
mPLUG-Owl3 & 0.74 & 0.06 & 0.64 & 0.04 & 0.68 & 0.04 & 0.79 & 0.07 & 0.75 & 0.03 & 0.77 & 0.05 & 0.67 & 0.08 & 0.55 & 0.04 & 0.61 & 0.05 & 0.63 & 0.13 & 0.46 & 0.06 & 0.53 & 0.08 & 0.70 & 0.07 & 0.68 & 0.04 & 0.69 & 0.05 \\
ViLAMP & 0.71 & 0.07 & 0.62 & 0.05 & 0.66 & 0.05 & 0.78 & 0.07 & 0.75 & 0.03 & 0.76 & 0.04 & 0.65 & 0.08 & 0.55 & 0.05 & 0.59 & 0.06 & 0.60 & 0.14 & 0.44 & 0.06 & 0.50 & 0.08 & 0.69 & 0.07 & 0.66 & 0.04 & 0.68 & 0.05 \\
LLaVA-OneVision & 0.70 & 0.06 & 0.62 & 0.04 & 0.65 & 0.04 & 0.79 & 0.05 & 0.74 & 0.03 & 0.76 & 0.04 & 0.65 & 0.07 & 0.54 & 0.05 & 0.59 & 0.05 & 0.64 & 0.07 & 0.44 & 0.06 & 0.52 & 0.06 & 0.68 & 0.07 & 0.67 & 0.04 & 0.67 & 0.05 \\
LongVU & 0.70 & 0.06 & 0.60 & 0.05 & 0.64 & 0.05 & 0.78 & 0.06 & 0.73 & 0.03 & 0.75 & 0.04 & 0.63 & 0.08 & 0.51 & 0.05 & 0.56 & 0.06 & 0.61 & 0.10 & 0.44 & 0.06 & 0.51 & 0.07 & 0.67 & 0.07 & 0.67 & 0.03 & 0.67 & 0.05 \\
Qwen2.5-72B & 0.66 & 0.07 & 0.59 & 0.04 & 0.63 & 0.05 & 0.77 & 0.06 & 0.72 & 0.03 & 0.75 & 0.04 & 0.63 & 0.08 & 0.50 & 0.05 & 0.56 & 0.06 & 0.59 & 0.09 & 0.43 & 0.06 & 0.50 & 0.06 & 0.68 & 0.07 & 0.65 & 0.04 & 0.66 & 0.05 \\
Qwen2.5-32B & 0.66 & 0.06 & 0.57 & 0.04 & 0.61 & 0.04 & 0.77 & 0.05 & 0.72 & 0.03 & 0.74 & 0.03 & 0.60 & 0.07 & 0.49 & 0.04 & 0.54 & 0.05 & 0.58 & 0.08 & 0.42 & 0.05 & 0.49 & 0.06 & 0.66 & 0.06 & 0.65 & 0.03 & 0.65 & 0.04 \\
VideoChat-Flash & 0.63 & 0.07 & 0.59 & 0.04 & 0.61 & 0.05 & 0.74 & 0.06 & 0.72 & 0.03 & 0.73 & 0.04 & 0.59 & 0.09 & 0.52 & 0.05 & 0.55 & 0.06 & 0.54 & 0.11 & 0.40 & 0.06 & 0.45 & 0.08 & 0.67 & 0.06 & 0.64 & 0.04 & 0.66 & 0.05 \\
MiniCPM-V & 0.67 & 0.06 & 0.55 & 0.04 & 0.60 & 0.04 & 0.78 & 0.05 & 0.70 & 0.03 & 0.74 & 0.03 & 0.61 & 0.07 & 0.47 & 0.04 & 0.53 & 0.05 & 0.60 & 0.07 & 0.40 & 0.05 & 0.48 & 0.06 & 0.66 & 0.06 & 0.64 & 0.03 & 0.65 & 0.04 \\
Video-XL & 0.63 & 0.08 & 0.57 & 0.05 & 0.60 & 0.06 & 0.74 & 0.06 & 0.72 & 0.03 & 0.73 & 0.04 & 0.56 & 0.09 & 0.49 & 0.04 & 0.52 & 0.06 & 0.55 & 0.09 & 0.40 & 0.06 & 0.47 & 0.06 & 0.62 & 0.08 & 0.65 & 0.04 & 0.63 & 0.05 \\
ShareGPT4Video & 0.64 & 0.07 & 0.57 & 0.05 & 0.60 & 0.05 & 0.72 & 0.07 & 0.70 & 0.03 & 0.71 & 0.05 & 0.55 & 0.08 & 0.48 & 0.05 & 0.51 & 0.06 & 0.55 & 0.11 & 0.40 & 0.06 & 0.46 & 0.08 & 0.61 & 0.07 & 0.64 & 0.04 & 0.62 & 0.05 \\
InternVL2 & 0.63 & 0.09 & 0.56 & 0.06 & 0.60 & 0.08 & 0.74 & 0.07 & 0.71 & 0.04 & 0.72 & 0.05 & 0.56 & 0.10 & 0.49 & 0.05 & 0.52 & 0.07 & 0.57 & 0.10 & 0.38 & 0.06 & 0.45 & 0.07 & 0.61 & 0.08 & 0.63 & 0.05 & 0.62 & 0.06 \\
TimeChat & 0.58 & 0.07 & 0.55 & 0.04 & 0.57 & 0.05 & 0.72 & 0.06 & 0.71 & 0.03 & 0.71 & 0.04 & 0.52 & 0.08 & 0.49 & 0.05 & 0.50 & 0.06 & 0.53 & 0.09 & 0.35 & 0.06 & 0.42 & 0.07 & 0.59 & 0.07 & 0.62 & 0.04 & 0.60 & 0.05 \\
LLaVA-NeXT-Video & 0.55 & 0.09 & 0.53 & 0.05 & 0.54 & 0.06 & 0.69 & 0.07 & 0.71 & 0.03 & 0.70 & 0.05 & 0.49 & 0.10 & 0.48 & 0.05 & 0.48 & 0.07 & 0.50 & 0.10 & 0.37 & 0.07 & 0.42 & 0.08 & 0.56 & 0.09 & 0.63 & 0.04 & 0.59 & 0.06 \\
TS-LLaVA & 0.54 & 0.10 & 0.51 & 0.06 & 0.53 & 0.07 & 0.68 & 0.08 & 0.69 & 0.04 & 0.68 & 0.06 & 0.46 & 0.11 & 0.46 & 0.06 & 0.46 & 0.08 & 0.47 & 0.11 & 0.35 & 0.07 & 0.40 & 0.08 & 0.56 & 0.09 & 0.62 & 0.05 & 0.58 & 0.07 \\
Oryx & 0.52 & 0.10 & 0.51 & 0.05 & 0.51 & 0.07 & 0.67 & 0.07 & 0.68 & 0.04 & 0.67 & 0.05 & 0.47 & 0.10 & 0.45 & 0.05 & 0.46 & 0.07 & 0.48 & 0.10 & 0.34 & 0.07 & 0.40 & 0.08 & 0.56 & 0.09 & 0.61 & 0.05 & 0.58 & 0.07 \\
LongVA & 0.50 & 0.10 & 0.46 & 0.07 & 0.48 & 0.08 & 0.62 & 0.08 & 0.66 & 0.04 & 0.64 & 0.06 & 0.41 & 0.10 & 0.41 & 0.06 & 0.40 & 0.07 & 0.40 & 0.10 & 0.31 & 0.07 & 0.35 & 0.08 & 0.49 & 0.10 & 0.57 & 0.05 & 0.53 & 0.08 \\
\bottomrule
\end{tabular}
}
\end{table*}

Across all embedding models, VideoLLaMA3 consistently achieves the highest scores, with KaLM-Gemma3-12B showing particularly strong discrimination (HM: 0.79). The standard deviations reveal model-specific variance patterns: NV-Embed-v2 exhibits the highest variability in coarse-grained scores (VideoLLaMA3: 0.10, mPLUG-Owl3: 0.13), while Llama-Embed-Nemotron-8B shows more conservative absolute scores but tighter distributions. This diversity across embedding models motivates our ensemble approach to reduce single-model bias.

\subsection{Overall VLM Ranking}

\Cref{tab:vlm_ranking} presents our final VLM ranking computed using Borda count aggregation across all five embedding models. For each embedding model, we rank VLMs by their harmonic mean score and assign Borda points (16 points for rank 1, 15 for rank 2, down to 0 for rank 17). The final ranking is determined by total Borda score, with MeanJudge (average HM across all five embedding models) serving as a tiebreaker.

\begin{table}[t]
\centering
\caption{Overall VLM ranking on CLIP-CC-Bench using Borda count aggregation across five MTEB embedding models. Borda scores represent cumulative ranking points across all judges. MeanJudge is the average harmonic mean score across all five embedding models.}
\label{tab:vlm_ranking}
\begin{tabular}{clccc}
\toprule
\textbf{Rank} & \textbf{VLM} & \textbf{Borda} & \textbf{MeanJudge} & \textbf{StdJudge} \\
\midrule
1 & VideoLLaMA3 & 80 & 0.67 & 0.09 \\
2 & mPLUG-Owl3 & 75 & 0.66 & 0.09 \\
3 & ViLAMP & 67 & 0.64 & 0.10 \\
4 & LLaVA-OneVision & 67 & 0.64 & 0.09 \\
5 & LongVU & 61 & 0.63 & 0.09 \\
6 & Qwen2.5-72B & 55 & 0.62 & 0.10 \\
7 & Qwen2.5-32B & 48 & 0.61 & 0.10 \\
8 & VideoChat-Flash & 42 & 0.60 & 0.10 \\
9 & MiniCPM-V & 42 & 0.60 & 0.10 \\
10 & Video-XL & 36 & 0.59 & 0.10 \\
11 & ShareGPT4Video & 29 & 0.58 & 0.10 \\
12 & InternVL2 & 27 & 0.58 & 0.10 \\
13 & TimeChat & 20 & 0.56 & 0.11 \\
14 & LLaVA-NeXT-Video & 16 & 0.55 & 0.11 \\
15 & TS-LLaVA & 9 & 0.53 & 0.11 \\
16 & Oryx & 6 & 0.52 & 0.11 \\
17 & LongVA & 0 & 0.48 & 0.11 \\
\bottomrule
\end{tabular}
\end{table}

VideoLLaMA3 achieves the highest Borda score (80 out of a maximum 80), demonstrating consistent top performance across all five embedding judges with a MeanJudge score of 0.67 (±0.09). mPLUG-Owl3 follows closely with a Borda score of 75 and MeanJudge of 0.66 (±0.09). ViLAMP and LLaVA-OneVision tie in Borda score (67) but are differentiated by their MeanJudge scores (0.64 each), though ViLAMP shows slightly higher variance (0.10 vs 0.09). The proprietary Qwen2.5 models (72B and 32B) demonstrate strong performance in the upper-middle tier, ranking 6th and 7th respectively, suggesting that general-purpose LLMs with visual capabilities can achieve competitive results on long-form video description when properly scaled.

\subsection{Analysis and Key Findings}

Our ensemble-based evaluation framework reveals several critical insights about long-form video description capabilities:

\textbf{1. Consistent Top Performers:} VideoLLaMA3's perfect Borda score (80/80) indicates unanimous agreement across all five embedding judges that it ranks first, suggesting robust long-form description capabilities that generalize across different semantic similarity metrics. The tight standard deviation (0.09) further confirms stable performance across diverse video content.

\textbf{2. Embedding Model Diversity:} While all five embedding models produce highly correlated rankings (as evidenced by consistent Borda scores), they exhibit different absolute score ranges and sensitivities. KaLM-Gemma3-12B consistently produces the highest absolute HM scores (VideoLLaMA3: 0.79), while NV-Embed-v2 shows the most conservative scoring (VideoLLaMA3: 0.55) but highest variance. This diversity justifies our ensemble approach for robust evaluation.

\textbf{3. Fine-Grained vs Coarse-Grained Gap:} Across all models, coarse-grained scores consistently exceed fine-grained scores, indicating that VLMs are better at capturing overall semantic meaning than precisely matching fine-grained details. For example, VideoLLaMA3 achieves 0.82 coarse but only 0.76 fine on KaLM-Gemma3-12B. This gap is most pronounced in NV-Embed-v2 (VideoLLaMA3: 0.68 coarse vs 0.47 fine), suggesting that detailed content matching remains a significant challenge.

\textbf{4. Performance Stratification:} Clear performance tiers emerge from our ranking: top-tier models (Borda ≥60) achieve MeanJudge scores above 0.63, mid-tier models (30 ≤ Borda < 60) range from 0.58-0.62, and lower-tier models (Borda < 30) fall below 0.58. This stratification suggests distinct capability levels rather than a continuous performance spectrum.

\textbf{5. Substantial Room for Improvement:} Even the best-performing model (VideoLLaMA3) achieves a MeanJudge score of only 0.67, with individual fine-grained scores as low as 0.47 on NV-Embed-v2. The substantial gap between top and bottom performers (0.67 vs 0.48) combined with modest absolute scores indicates significant headroom for advancement in long-form video description capabilities.

\textbf{6. Model Size and Architecture:} The strong performance of Qwen2.5-72B and Qwen2.5-32B (ranks 6-7) demonstrates that general-purpose LLMs with visual capabilities can compete with specialized video-language models. However, VideoLLaMA3's superior performance suggests that specialized video understanding mechanisms remain advantageous for long-form paragraph-level description tasks.
