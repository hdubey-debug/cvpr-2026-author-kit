\section{Results}
\label{sec:results}

We present comprehensive evaluation results for 17 state-of-the-art Video Language Models on CLIP-CC-Bench using our ensemble-based evaluation framework. Our methodology employs five leading LLM-based embedding models from the MTEB~\cite{muennighoff2023mteb} leaderboard: GTE-Qwen2-7B~\cite{li2024gte}, KaLM-Embedding-Gemma3-12B~\cite{kalm2024}, Llama-Embed-Nemotron-8B~\cite{nvidia2024nemotron}, nv-embed-v2~\cite{lee2024nv}, and Qwen3-Embedding-8B~\cite{qwen2024embedding}. For each VLM, we compute coarse-grained semantic similarity (paragraph-level alignment) and fine-grained semantic similarity (sentence-level F1 matching) across all five embedding models. We then rank VLMs using Borda count based on their harmonic mean (HM) of coarse and fine scores for each embedding model, and report mean HM scores averaged across all five judges.

\subsection{Per-Judge Evaluation Statistics}

\Cref{tab:detailed_stats} presents detailed performance statistics for all 17 VLMs across the five embedding models. For each embedding model, we report coarse-grained similarity scores, fine-grained F1 scores, and their harmonic mean, along with standard deviations. The hierarchical structure reveals how different embedding models assess video description quality with varying sensitivities.

\begin{table*}[t]
\centering
\caption{Detailed evaluation statistics across five MTEB embedding models. For each model, we report Coarse (coarse-grained similarity), Fine (fine-grained F1), and HM (harmonic mean) as mean±std.}
\label{tab:detailed_stats}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|rrr|rrr|rrr|rrr|rrr}
\toprule
& \multicolumn{3}{c|}{\textbf{GTE-Qwen2-7B}} & \multicolumn{3}{c|}{\textbf{KaLM-Gemma3-12B}} & \multicolumn{3}{c|}{\textbf{Nemo-8B}} & \multicolumn{3}{c|}{\textbf{NV-Embed-v2}} & \multicolumn{3}{c}{\textbf{Qwen3-8B}} \\
\textbf{VLM} & \rotatebox{90}{\textbf{Coarse}} & \rotatebox{90}{\textbf{Fine}} & \rotatebox{90}{\textbf{HM}} & \rotatebox{90}{\textbf{Coarse}} & \rotatebox{90}{\textbf{Fine}} & \rotatebox{90}{\textbf{HM}} & \rotatebox{90}{\textbf{Coarse}} & \rotatebox{90}{\textbf{Fine}} & \rotatebox{90}{\textbf{HM}} & \rotatebox{90}{\textbf{Coarse}} & \rotatebox{90}{\textbf{Fine}} & \rotatebox{90}{\textbf{HM}} & \rotatebox{90}{\textbf{Coarse}} & \rotatebox{90}{\textbf{Fine}} & \rotatebox{90}{\textbf{HM}} \\
\midrule
VideoLLaMA3 & 0.75±0.05 & 0.64±0.04 & 0.69±0.04 & 0.82±0.05 & 0.76±0.03 & 0.79±0.03 & 0.68±0.07 & 0.57±0.04 & 0.62±0.05 & 0.68±0.10 & 0.47±0.07 & 0.55±0.08 & 0.72±0.06 & 0.69±0.03 & 0.70±0.04 \\
mPLUG-Owl3 & 0.74±0.06 & 0.64±0.04 & 0.68±0.04 & 0.79±0.07 & 0.75±0.03 & 0.77±0.05 & 0.67±0.08 & 0.55±0.04 & 0.61±0.05 & 0.63±0.13 & 0.46±0.06 & 0.53±0.08 & 0.70±0.07 & 0.68±0.04 & 0.69±0.05 \\
ViLAMP & 0.71±0.07 & 0.62±0.05 & 0.66±0.05 & 0.78±0.07 & 0.75±0.03 & 0.76±0.04 & 0.65±0.08 & 0.55±0.05 & 0.59±0.06 & 0.60±0.14 & 0.44±0.06 & 0.50±0.08 & 0.69±0.07 & 0.66±0.04 & 0.68±0.05 \\
LLaVA-OneVision & 0.70±0.06 & 0.62±0.04 & 0.65±0.04 & 0.79±0.05 & 0.74±0.03 & 0.76±0.04 & 0.65±0.07 & 0.54±0.05 & 0.59±0.05 & 0.64±0.07 & 0.44±0.06 & 0.52±0.06 & 0.68±0.07 & 0.67±0.04 & 0.67±0.05 \\
LongVU & 0.70±0.06 & 0.60±0.05 & 0.64±0.05 & 0.78±0.06 & 0.73±0.03 & 0.75±0.04 & 0.63±0.08 & 0.51±0.05 & 0.56±0.06 & 0.61±0.10 & 0.44±0.06 & 0.51±0.07 & 0.67±0.07 & 0.67±0.03 & 0.67±0.05 \\
Qwen2.5-72B & 0.66±0.07 & 0.59±0.04 & 0.63±0.05 & 0.77±0.06 & 0.72±0.03 & 0.75±0.04 & 0.63±0.08 & 0.50±0.05 & 0.56±0.06 & 0.59±0.09 & 0.43±0.06 & 0.50±0.06 & 0.68±0.07 & 0.65±0.04 & 0.66±0.05 \\
Qwen2.5-32B & 0.66±0.06 & 0.57±0.04 & 0.61±0.04 & 0.77±0.05 & 0.72±0.03 & 0.74±0.03 & 0.60±0.07 & 0.49±0.04 & 0.54±0.05 & 0.58±0.08 & 0.42±0.05 & 0.49±0.06 & 0.66±0.06 & 0.65±0.03 & 0.65±0.04 \\
VideoChat-Flash & 0.63±0.07 & 0.59±0.04 & 0.61±0.05 & 0.74±0.06 & 0.72±0.03 & 0.73±0.04 & 0.59±0.09 & 0.52±0.05 & 0.55±0.06 & 0.54±0.11 & 0.40±0.06 & 0.45±0.08 & 0.67±0.06 & 0.64±0.04 & 0.66±0.05 \\
MiniCPM-V & 0.67±0.06 & 0.55±0.04 & 0.60±0.04 & 0.78±0.05 & 0.70±0.03 & 0.74±0.03 & 0.61±0.07 & 0.47±0.04 & 0.53±0.05 & 0.60±0.07 & 0.40±0.05 & 0.48±0.06 & 0.66±0.06 & 0.64±0.03 & 0.65±0.04 \\
Video-XL & 0.63±0.08 & 0.57±0.05 & 0.60±0.06 & 0.74±0.06 & 0.72±0.03 & 0.73±0.04 & 0.56±0.09 & 0.49±0.04 & 0.52±0.06 & 0.55±0.09 & 0.40±0.06 & 0.47±0.06 & 0.62±0.08 & 0.65±0.04 & 0.63±0.05 \\
ShareGPT4Video & 0.64±0.07 & 0.57±0.05 & 0.60±0.05 & 0.72±0.07 & 0.70±0.03 & 0.71±0.05 & 0.55±0.08 & 0.48±0.05 & 0.51±0.06 & 0.55±0.11 & 0.40±0.06 & 0.46±0.08 & 0.61±0.07 & 0.64±0.04 & 0.62±0.05 \\
InternVL2 & 0.63±0.09 & 0.56±0.06 & 0.60±0.08 & 0.74±0.07 & 0.71±0.04 & 0.72±0.05 & 0.56±0.10 & 0.49±0.05 & 0.52±0.07 & 0.57±0.10 & 0.38±0.06 & 0.45±0.07 & 0.61±0.08 & 0.63±0.05 & 0.62±0.06 \\
TimeChat & 0.58±0.07 & 0.55±0.04 & 0.57±0.05 & 0.72±0.06 & 0.71±0.03 & 0.71±0.04 & 0.52±0.08 & 0.49±0.05 & 0.50±0.06 & 0.53±0.09 & 0.35±0.06 & 0.42±0.07 & 0.59±0.07 & 0.62±0.04 & 0.60±0.05 \\
LLaVA-NeXT-Video & 0.55±0.09 & 0.53±0.05 & 0.54±0.06 & 0.69±0.07 & 0.71±0.03 & 0.70±0.05 & 0.49±0.10 & 0.48±0.05 & 0.48±0.07 & 0.50±0.10 & 0.37±0.07 & 0.42±0.08 & 0.56±0.09 & 0.63±0.04 & 0.59±0.06 \\
TS-LLaVA & 0.54±0.10 & 0.51±0.06 & 0.53±0.07 & 0.68±0.08 & 0.69±0.04 & 0.68±0.06 & 0.46±0.11 & 0.46±0.06 & 0.46±0.08 & 0.47±0.11 & 0.35±0.07 & 0.40±0.08 & 0.56±0.09 & 0.62±0.05 & 0.58±0.07 \\
Oryx & 0.52±0.10 & 0.51±0.05 & 0.51±0.07 & 0.67±0.07 & 0.68±0.04 & 0.67±0.05 & 0.47±0.10 & 0.45±0.05 & 0.46±0.07 & 0.48±0.10 & 0.34±0.07 & 0.40±0.08 & 0.56±0.09 & 0.61±0.05 & 0.58±0.07 \\
LongVA & 0.50±0.10 & 0.46±0.07 & 0.48±0.08 & 0.62±0.08 & 0.66±0.04 & 0.64±0.06 & 0.41±0.10 & 0.41±0.06 & 0.40±0.07 & 0.40±0.10 & 0.31±0.07 & 0.35±0.08 & 0.49±0.10 & 0.57±0.05 & 0.53±0.08 \\
\bottomrule
\end{tabular}
}
\end{table*}

Across all embedding models, VideoLLaMA3 consistently achieves the highest scores, with KaLM-Gemma3-12B showing particularly strong discrimination (HM: 0.79). The standard deviations reveal model-specific variance patterns: NV-Embed-v2 exhibits the highest variability in coarse-grained scores (VideoLLaMA3: 0.10, mPLUG-Owl3: 0.13), while Llama-Embed-Nemotron-8B shows more conservative absolute scores but tighter distributions. This diversity across embedding models motivates our ensemble approach to reduce single-model bias.

\subsection{Overall VLM Ranking}

\Cref{tab:vlm_ranking} presents our final VLM ranking computed using Borda count aggregation across all five embedding models. For each embedding model, we rank VLMs by their harmonic mean score and assign Borda points (16 points for rank 1, 15 for rank 2, down to 0 for rank 17). The final ranking is determined by total Borda score, with Mean (average HM across all five embedding models) serving as a tiebreaker.

\begin{table}[t]
\centering
\caption{Overall VLM ranking on CLIP-CC-Bench using Borda count aggregation across five MTEB embedding models. Borda scores represent cumulative ranking points across all judges. Mean is the average harmonic mean score across all five embedding models.}
\label{tab:vlm_ranking}
\begin{tabular}{clcc}
\toprule
\textbf{Rank} & \textbf{VLM} & \textbf{Borda} & \textbf{Mean} \\
\midrule
1 & VideoLLaMA3 & 80 & 0.67 \\
2 & mPLUG-Owl3 & 75 & 0.66 \\
3 & ViLAMP & 67 & 0.64 \\
4 & LLaVA-OneVision & 67 & 0.64 \\
5 & LongVU & 61 & 0.63 \\
6 & Qwen2.5-72B & 55 & 0.62 \\
7 & Qwen2.5-32B & 48 & 0.61 \\
8 & VideoChat-Flash & 42 & 0.60 \\
9 & MiniCPM-V & 42 & 0.60 \\
10 & Video-XL & 36 & 0.59 \\
11 & ShareGPT4Video & 29 & 0.58 \\
12 & InternVL2 & 27 & 0.58 \\
13 & TimeChat & 20 & 0.56 \\
14 & LLaVA-NeXT-Video & 16 & 0.55 \\
15 & TS-LLaVA & 9 & 0.53 \\
16 & Oryx & 6 & 0.52 \\
17 & LongVA & 0 & 0.48 \\
\bottomrule
\end{tabular}
\end{table}

VideoLLaMA3 achieves the highest Borda score (80 out of maximum 80), demonstrating consistent top performance across all five embedding judges with a mean score of 0.67. mPLUG-Owl3 follows closely (Borda: 75, Mean: 0.66), while ViLAMP and LLaVA-OneVision tie in Borda score (67) with identical mean scores (0.64). The Qwen2.5 models (72B and 32B) demonstrate strong upper-middle tier performance (ranks 6-7), indicating general-purpose LLMs with visual capabilities can achieve competitive results on long-form video description when properly scaled.

\subsection{Analysis and Key Findings}

Our ensemble-based evaluation framework reveals critical insights about long-form video description capabilities:

\textbf{1. Consistent Top Performers:} VideoLLaMA3's perfect Borda score (80/80) indicates unanimous first-place agreement across all five embedding judges, demonstrating robust long-form description capabilities that generalize across different semantic similarity metrics.

\textbf{2. Embedding Model Diversity:} While embedding models produce correlated rankings, they exhibit different absolute score ranges and sensitivities. KaLM-Gemma3-12B produces the highest HM scores (VideoLLaMA3: 0.79), while NV-Embed-v2 shows conservative scoring (VideoLLaMA3: 0.55) but highest variance, justifying our ensemble approach.

\textbf{3. Fine-Grained vs Coarse-Grained Gap:} Coarse-grained scores consistently exceed fine-grained scores across all models, indicating VLMs capture overall semantic meaning better than fine-grained details. VideoLLaMA3 achieves 0.82 coarse vs 0.76 fine on KaLM-Gemma3-12B. This gap is most pronounced in NV-Embed-v2 (0.68 coarse vs 0.47 fine), indicating detailed content matching remains challenging.

\textbf{4. Architecture Family Performance:} Transformer-based models dominate the top tier, with VideoLLaMA3 (rank 1), mPLUG-Owl3 (rank 2), and LLaVA-OneVision (rank 4) achieving Borda scores ≥67. Multimodal family models show competitive performance (ViLAMP at rank 3, ShareGPT4Video at rank 11). Efficient architectures demonstrate moderate performance (LongVU at rank 5, MiniCPM-V at rank 9), while specialized temporal models underperform (TimeChat, TS-LLaVA, Video-XL at ranks 10-15), suggesting temporal modeling alone is insufficient for long-form description without strong semantic understanding.

\textbf{5. Performance Stratification:} Clear performance tiers emerge: top-tier models (Borda ≥60) achieve mean scores above 0.63, mid-tier models (30 ≤ Borda < 60) range from 0.58-0.62, and lower-tier models (Borda < 30) fall below 0.58, suggesting distinct capability levels rather than continuous performance.

\textbf{6. Substantial Improvement Headroom:} Even top-performing VideoLLaMA3 achieves only 0.67 mean score, with individual fine-grained scores as low as 0.47 on NV-Embed-v2. The substantial gap between top and bottom performers (0.67 vs 0.48) combined with modest absolute scores indicates significant advancement potential in long-form video description.
