\section{Conclusion}
\label{sec:conclusion}

This work introduces CLIP-CC-Bench, an evaluation suite for long-form, paragraph-level video description assessment. Our framework addresses critical gaps in current benchmarking by employing an ensemble of five state-of-the-art LLM-based embedding models from MTEB, applying complementary coarse-grained and fine-grained semantic matching methodologies across 200 carefully curated 90-second movie clips with expert-generated descriptions.

Our evaluation of 17 state-of-the-art Vision Language Models using Borda count aggregation reveals that transformer-based architectures dominate long-form video description, with VideoLLaMA3 achieving perfect consensus (Borda: 80/80) across all embedding judges. The consistent coarse-fine granularity gap across all models—where coarse-grained scores substantially exceed fine-grained scores—demonstrates that current VLMs capture overall semantic meaning more effectively than precise fine-grained details. Architecture family analysis shows transformer models excel (ranks 1-4), multimodal models demonstrate competitive performance (ranks 3, 11), while specialized temporal models underperform (ranks 10-15), indicating temporal modeling alone is insufficient without strong semantic understanding.

Our ensemble approach mitigates single-model bias through multi-judge consensus, revealing that different embedding models exhibit varying sensitivities and score ranges while maintaining correlated rankings. The modest absolute performance even for top models (VideoLLaMA3: 0.67 mean) combined with the substantial performance gap (0.67 vs 0.48) indicates significant advancement potential in long-form video description capabilities.

By providing standardized evaluation protocols through ensemble-based semantic matching, reproducible Borda count ranking, and systematic proper noun elimination to focus on fundamental visual understanding, CLIP-CC-Bench establishes a practical framework for evaluating long-form video description that complements existing short-clip and QA-based benchmarks. All resources, including evaluation scripts, model outputs, and aggregation tools, are publicly available to facilitate continued progress in video-language understanding.
