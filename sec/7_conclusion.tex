\section{Conclusion}
\label{sec:conclusion}

We introduce CLIP-CC-Bench, an evaluation suite for long-form video description using an ensemble of five MTEB embedding models with Borda count aggregation. Evaluation of 17 VLMs reveals transformer architectures dominate (VideoLLaMA3 achieves perfect 80/80 Borda consensus), while specialized temporal models underperform, indicating temporal modeling alone is insufficient. The consistent coarse-fine granularity gap shows VLMs capture overall semantics better than precise details, with substantial improvement headroom (top model: 0.67 mean). All evaluation resources are publicly available.
