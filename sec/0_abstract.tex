\begin{abstract}
Benchmarking video-language models has largely centered on short clips and single-sentence metrics, leaving open whether systems can generate accurate long-form, paragraph-level descriptions. We introduce CLIP-CC Bench, an evaluation suite for long-form video description built on 200 approximately 90-second movie clips paired with expert-written reference paragraphs. Our evaluation suite employs three complementary families: (i) N-gram overlap metrics to verify lexical alignment; (ii) embedding-based semantic similarity using state-of-the-art encoders to capture meaning beyond surface forms; and (iii) LLM-as-judge assessment using the ACCR rubric (Accuracy, Completeness, Conciseness, Relevance) to evaluate discourse quality. We evaluate 14 state-of-the-art Vision Language Models, revealing substantial divergence across evaluation families. Models excel in different dimensions, with architectural design and training methodology proving more influential than parameter count. The dataset comprises 200 curated clips with ground-truth descriptions averaging 402 words, and we release standardized evaluation scripts, model outputs, and aggregation tools for reproducibility. CLIP-CC Bench provides a practical evaluation framework for long-form video description capabilities not addressed by existing short-clip or QA-only benchmarks.
\end{abstract}
