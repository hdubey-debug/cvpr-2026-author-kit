\begin{abstract}
Benchmarking video-language models has largely centered on short clips and single-sentence metrics, leaving open whether systems can generate accurate long-form, paragraph-level descriptions. We introduce CLIP-CC Bench, an evaluation suite for long-form video description built on 5 hours of movie content broken down into 90-second clips paired with expert-written long-form paragraph-style references. Our evaluation suite employs an ensemble of 5 state-of-the-art LLM-based embedding models to increase reliability and mitigate single-model bias, and applies 2 complementary methodologies: (i) coarse-grained semantic matching and (ii) fine-grained semantic matching, to compare model-generated descriptions against the CLIP-CC Bench references. Using this evaluation suite, we evaluate 17 state-of-the-art video-language models and report both their Borda-aggregated ranking and their average scores on CLIP-CC Bench. We release standardized evaluation scripts, model outputs, and aggregation tools for reproducibility. CLIP-CC Bench provides a practical evaluation framework for long-form video description capabilities not addressed by existing short-clip or QA-only benchmarks.
\end{abstract}
